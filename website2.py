import os
import streamlit as st
import pickle
import config  # Import your config module
import google.generativeai as genai
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.document_loaders import UnstructuredURLLoader
from langchain.vectorstores import FAISS
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.schema import Document
import pdfplumber
import pytesseract  # OCR for scanned PDFs
from PIL import Image
from langchain.chains import RetrievalQA
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.prompts import PromptTemplate

# ‚úÖ Securely Fetch API Key
if not config.GEMINI_API_KEY:
    st.error("‚ö†Ô∏è Please set the GEMINI_API_KEY in the .env file!")
    st.stop()

genai.configure(api_key=config.GEMINI_API_KEY)

# ‚úÖ Sidebar for User Input
st.sidebar.markdown("### üìú Input Article URLs or PDF")
urls = [st.sidebar.text_input(f"üìù URL {i+1}") for i in range(3)]
pdf_file = st.sidebar.file_uploader("üìÑ Upload PDF", type="pdf")
process_clicked = st.sidebar.button("üîç Process URLs/PDF")

# ‚úÖ FAISS Storage Path from Config
file_path = config.FAISS_FILE_PATH

# ‚úÖ Load URLs with Retry Logic
def load_data_with_retry(urls, retries=3):
    data = []
    for url in urls:
        if not url.strip():
            continue
        for attempt in range(retries):
            try:
                loader = UnstructuredURLLoader(urls=[url])
                documents = loader.load()
                data.extend(documents)
                st.success(f"‚úÖ Loaded data from {url}")
                break
            except Exception as e:
                st.warning(f"Retry {attempt+1}: Failed to load {url}. Error: {e}")
    return data

# ‚úÖ Load PDF with Improved OCR Handling
def load_pdf(pdf_file):
    data = []
    try:
        with pdfplumber.open(pdf_file) as pdf:
            for page in pdf.pages:
                text = page.extract_text()
                if not text:
                    image = page.to_image(resolution=300)
                    text = pytesseract.image_to_string(Image.open(image))
                
                if text:
                    data.append(Document(page_content=text))
    except Exception as e:
        st.error(f"Error processing PDF: {e}")
    return data

# ‚úÖ Process URLs and PDFs
if process_clicked:
    valid_urls = [url for url in urls if url]
    if not valid_urls and not pdf_file:
        st.error("‚ö†Ô∏è Provide at least one URL or upload a PDF!")
    else:
        st.info("‚è≥ Processing documents...")

        data = []
        if valid_urls:
            data.extend(load_data_with_retry(valid_urls))

        if pdf_file:
            data.extend(load_pdf(pdf_file))

        if data:
            with st.spinner("üîç Creating Embeddings..."):
                text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
                docs = text_splitter.split_documents(data)

                embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
                vectorstore = FAISS.from_documents(docs, embeddings)

                with open(file_path, "wb") as f:
                    pickle.dump(vectorstore, f)
                
                st.success(f"‚úÖ FAISS index saved at: {file_path}")
        else:
            st.error("‚ùå No valid data found. Check your URLs or PDF file.")

# ‚úÖ Query Section
st.markdown("### üîé Ask a Question")
query = st.text_input("üí° Enter your question here:")

if query:
    if os.path.exists(file_path):
        with st.spinner("üîç Searching..."):
            with open(file_path, "rb") as f:
                vectorstore = pickle.load(f)

            llm = ChatGoogleGenerativeAI(model="gemini-pro", temperature=0)

            template = """
            Use the following context to answer the question at the end.
            If the context doesn't contain the answer, say "I cannot answer this based on the provided information."
            Be concise and cite the sources.

            Context: {context}

            Question: {question}
            """
            PROMPT = PromptTemplate(template=template, input_variables=["context", "question"])

            qa_chain = RetrievalQA.from_chain_type(
                llm=llm,
                chain_type="stuff",
                retriever=vectorstore.as_retriever(),
                chain_type_kwargs={"prompt": PROMPT},
                return_source_documents=True,
            )

            result = qa_chain({"query": query})

            # ‚úÖ Display Answer
            st.markdown("### üß† Answer")
            st.write(result["result"])

            # ‚úÖ Display Sources
            st.markdown("### üìö Sources")
            seen_sources = set()
            for i, doc in enumerate(result["source_documents"], start=1):
                source = doc.metadata.get('source', 'Unknown Source')
                if source not in seen_sources:
                    st.write(f"{i}. {source}")
                    seen_sources.add(source)

    else:
        st.error(f"‚ùå FAISS index not found at {file_path}. Process URLs first.")

# ‚úÖ Footer
st.markdown(
    """
    <div style="text-align: center; font-size: 16px;">
        üõ†Ô∏è <b style="color: #2196F3;">Developed by RK</b> |
        ‚úÖ <b style="color: #4CAF50;">Powered by Gemini Pro, LangChain & FAISS</b> |
        üöÄ <b style="color: #FF9800;">Built with Streamlit</b>
    </div>
    """,
    unsafe_allow_html=True,
)
